# bot.py
from pyrogram import Client, filters
from dotenv import load_dotenv
import os
import sys
import re
import requests
import traceback
from collections import defaultdict
from io import BytesIO
import logging
import signal

# ---------- –õ–û–ì–ò ----------
logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")
log = logging.getLogger("bot")

# ---------- –û–ö–†–£–ñ–ï–ù–ò–ï ----------
load_dotenv()

BOT_TOKEN = os.getenv("BOT_TOKEN")
API_ID_STR = os.getenv("API_ID")
API_HASH = os.getenv("API_HASH")

# OpenRouter (—Ç–µ–∫—Å—Ç + –ø–µ—Ä–µ–≤–æ–¥)
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
# –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è/–¥–µ—à–µ–≤–∞—è —Ç–µ–∫—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞/–ø–µ—Ä–µ–≤–æ–¥–∞
OR_MODEL = os.getenv("OR_TEXT_MODEL", "openai/gpt-oss-120b")

# Hugging Face (–∫–∞—Ä—Ç–∏–Ω–∫–∏)
HF_TOKEN = os.getenv("HF_TOKEN")
# —Ä–∞–±–æ—á–∞—è –ø—É–±–ª–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å; –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ Railway
HF_IMAGE_MODEL = os.getenv("HF_IMAGE_MODEL", "stabilityai/sdxl-turbo")

missing = [k for k, v in {
    "BOT_TOKEN": BOT_TOKEN,
    "API_ID": API_ID_STR,
    "API_HASH": API_HASH,
    "OPENROUTER_API_KEY": OPENROUTER_API_KEY,
    "HF_TOKEN": HF_TOKEN,
}.items() if not v]
if missing:
    log.error("‚ùå –ù–µ –∑–∞–¥–∞–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: %s", ", ".join(missing))
    sys.exit(1)

try:
    API_ID = int(API_ID_STR)
except Exception:
    log.error("‚ùå API_ID –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º, –ø–æ–ª—É—á–µ–Ω–æ: %r", API_ID_STR)
    sys.exit(1)

# ---------- –£–¢–ò–õ–ò–¢–´ ----------
def or_headers(title: str = "TelegramBot"):
    return {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "Accept": "application/json",
        "HTTP-Referer": "https://openrouter.ai",
        "X-Title": title,
    }

def has_cyrillic(text: str) -> bool:
    return bool(re.search(r"[\u0400-\u04FF]", text))

def translate_to_english(text: str) -> str:
    """
    –ü–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ OpenRouter –±–µ–∑ –≤–æ–¥—ã ‚Äî —Ç–æ–ª—å–∫–æ –≥–æ—Ç–æ–≤–∞—è —Ñ—Ä–∞–∑–∞.
    """
    try:
        payload = {
            "model": OR_MODEL,
            "messages": [
                {"role": "system", "content": (
                    "You are a precise translator. "
                    "Translate the user prompt from Russian to concise English. "
                    "Return ONLY the translated text, no explanations."
                )},
                {"role": "user", "content": text}
            ],
            "temperature": 0.2
        }
        r = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=or_headers("PromptTranslator"),
            json=payload,
            timeout=40,
            allow_redirects=False
        )
        if r.status_code == 200 and r.headers.get("content-type","").startswith("application/json"):
            return r.json()["choices"][0]["message"]["content"].strip()
        log.warning("Translate: %s | %s", r.status_code, r.text[:300])
    except Exception:
        traceback.print_exc()
    # –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫ ‚Äî –≤–µ—Ä–Ω—ë–º –∏—Å—Ö–æ–¥–Ω—ã–π
    return text

def boost_prompt(en_prompt: str, user_negative: str = "") -> tuple[str, str]:
    """
    –£—Å–∏–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º (positive_prompt, negative_prompt)
    """
    base_positive = (
        f"{en_prompt}, ultra-detailed, high quality, high resolution, "
        f"sharp focus, intricate details, 8k, dramatic lighting"
    )
    base_negative = (
        "lowres, blurry, out of focus, pixelated, deformed, bad anatomy, "
        "extra fingers, watermark, text, signature"
    )
    # –¥–æ–±–∞–≤–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π negative (–µ—Å–ª–∏ –µ—Å—Ç—å)
    neg = (base_negative + ", " + user_negative) if user_negative else base_negative
    return base_positive, neg

# ---------- –ü–ê–ú–Ø–¢–¨ –î–ò–ê–õ–û–ì–ê ----------
chat_history = defaultdict(list)
HISTORY_LIMIT = 10

def clamp_history(history):
    return history[-HISTORY_LIMIT:] if len(history) > HISTORY_LIMIT else history

# ---------- PYROGRAM ----------
app = Client("my_bot", bot_token=BOT_TOKEN, api_id=API_ID, api_hash=API_HASH)

# ---------- –ö–û–ú–ê–ù–î–´ ----------
@app.on_message(filters.command("start"))
def start_handler(_, message):
    uid = message.from_user.id
    chat_history[uid] = []
    message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç —Å –ø–∞–º—è—Ç—å—é ü§ñ\n"
        "‚Äî –ü–∏—à–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: —É—á–∏—Ç—ã–≤–∞—é –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 10 —Ä–µ–ø–ª–∏–∫.\n"
        "‚Äî –ö–∞—Ä—Ç–∏–Ω–∫–∞: /img –∫–æ—Ç –≤ –∫–æ—Å–º–æ—Å–µ  (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç  --no ...  –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏–π)\n"
        "‚Äî –û—á–∏—Å—Ç–∏—Ç—å –ø–∞–º—è—Ç—å: /reset"
    )

@app.on_message(filters.command("reset"))
def reset_handler(_, message):
    uid = message.from_user.id
    chat_history[uid] = []
    message.reply_text("üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞!")

# ---------- –¢–ï–ö–°–¢ –° –ü–ê–ú–Ø–¢–¨–Æ (OpenRouter) ----------
@app.on_message(filters.text & ~filters.command(["start", "reset", "img"]))
def text_handler(_, message):
    uid = message.from_user.id
    user_text = message.text

    chat_history[uid].append({"role": "user", "content": user_text})
    chat_history[uid] = clamp_history(chat_history[uid])

    try:
        payload = {
            "model": OR_MODEL,
            "messages": [
                {"role": "system", "content": "–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π Telegram-–±–æ—Ç. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."},
                *chat_history[uid],
            ],
        }

        resp = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=or_headers("TelegramBotWithMemory"),
            json=payload,
            timeout=60,
            allow_redirects=False
        )

        log.info("TEXT %s | %s", resp.status_code, resp.headers.get("content-type", ""))
        if resp.status_code != 200:
            snippet = (resp.text or "")[:600]
            message.reply_text(f"‚ùå OpenRouter {resp.status_code}\n{snippet}")
            return

        data = resp.json()
        bot_reply = data["choices"][0]["message"]["content"].strip()

        chat_history[uid].append({"role": "assistant", "content": bot_reply})
        chat_history[uid] = clamp_history(chat_history[uid])

        message.reply_text(bot_reply or "ü§ñ (–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç)")

    except Exception:
        traceback.print_exc()
        message.reply_text("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—â–µ–Ω–∏–∏ —Å OpenRouter ü§ñ")

# ---------- –ö–ê–†–¢–ò–ù–ö–ò /img (HF Inference API + –ø–µ—Ä–µ–≤–æ–¥ + –±—É—Å—Ç) ----------
@app.on_message(filters.command("img"))
def image_handler(_, message):
    """
    –ü–æ–¥–¥–µ—Ä–∂–∫–∞:
    /img –∫–æ—Ç –≤ –∫–æ—Å–º–æ—Å–µ --no —Ç–µ–∫—Å—Ç, –ø–æ–¥–ø–∏—Å–∏
    """
    raw = " ".join(message.command[1:]).strip()
    if not raw:
        message.reply_text(
            "–ù–∞–ø–∏—à–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ—Å–ª–µ –∫–æ–º–∞–Ω–¥—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n"
            "/img –∫–æ—Ç –≤ –∫–æ—Å–º–æ—Å–µ, –Ω–µ–æ–Ω, 4k  --no —Ç–µ–∫—Å—Ç, –ø–æ–¥–ø–∏—Å–∏"
        )
        return

    # —Ä–∞–∑–±–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ negative –ø–æ—Å–ª–µ "--no"
    user_neg = ""
    if "--no" in raw:
        parts = raw.split("--no", 1)
        raw = parts[0].strip()
        user_neg = parts[1].strip()

    # –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
    prompt_src = raw
    prompt_en = translate_to_english(raw) if has_cyrillic(raw) else raw

    # –±—É—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
    pos_prompt, neg_prompt = boost_prompt(prompt_en, user_negative=user_neg)

    try:
        url = f"https://api-inference.huggingface.co/models/{HF_IMAGE_MODEL}"
        headers = {
            "Authorization": f"Bearer {HF_TOKEN}",
            "Accept": "image/png"
        }
        # –º–Ω–æ–≥–∏–µ t2i –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∏–∂–µ (–º–æ–≥—É—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å)
        payload = {
            "inputs": pos_prompt,
            "parameters": {
                "negative_prompt": neg_prompt,
                "num_inference_steps": 24,
                "guidance_scale": 7.0
            },
            "options": {"wait_for_model": True}
        }

        resp = requests.post(url, headers=headers, json=payload, timeout=180)
        ct = resp.headers.get("content-type", "")
        log.info("IMG %s | %s", resp.status_code, ct)

        if resp.status_code == 200 and ct.startswith("image/"):
            bio = BytesIO(resp.content)
            bio.name = "image.png"
            # –ø–æ–∫–∞–∂–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ä—É—Å—Å–∫–∏–π (–µ—Å–ª–∏ –±—ã–ª) –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
            shown_prompt = prompt_src if prompt_src else prompt_en
            message.reply_photo(bio, caption=f"üé® –ü–æ –∑–∞–ø—Ä–æ—Å—É: {shown_prompt}")
            return

        snippet = (resp.text or "")[:800]
        message.reply_text(
            "‚ùå Hugging Face {code}\n–ú–æ–¥–µ–ª—å: {model}\nURL: {url}\n\n{snippet}".format(
                code=resp.status_code, model=HF_IMAGE_MODEL, url=url, snippet=snippet
            )
        )

    except Exception:
        traceback.print_exc()
        message.reply_text("–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è üé®")

# ---------- –ì–†–ê–¶–ò–û–ó–ù–û–ï –ó–ê–í–ï–†–®–ï–ù–ò–ï ----------
def _graceful_exit(sig, frame):
    logging.getLogger().info("Stop signal received (%s). Exiting...", sig)
    try:
        app.stop()
    finally:
        os._exit(0)

signal.signal(signal.SIGTERM, _graceful_exit)
signal.signal(signal.SIGINT, _graceful_exit)

# ---------- –ó–ê–ü–£–°–ö ----------
if __name__ == "__main__":
    try:
        log.info("‚úÖ –ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
        app.run()
    except Exception:
        traceback.print_exc()
        sys.exit(1)














